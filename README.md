# ML-Neural-Networks
Write a python code in Jupyter notebook to build the following fully connected feedforward neural
networks for digit classification on MNIST data and compare their performance. In both the setup, the
network must have 10 hidden layers, each layer having 10 neurons.
1. The activation function is sigmoid everywhere.
2. The activation function is ReLU (except in the output layer, where the activation function is
sigmoid)
Train your network using ADAM optimizer, with 64 batch size, learning rate 0.0001 and error function is
cross entropy. Use batch normalization. You must train and test your model on the training set and the
test set given in the following link: http://yann.lecun.com/exdb/mnist/. 
